{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JJ: Import required modules\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# add workspace to sys.path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
        "\n",
        "from qwen_vl_utils import process_vision_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # JJ: Define prepare_spatial_mllm_inputs function\n",
        "# def prepare_spatial_mllm_inputs(batch, video_inputs, image_inputs):\n",
        "#     \"\"\"\n",
        "#         Prepare inputs for Spatial MLLM model.\n",
        "#         Batch: Dict return by the processor\n",
        "#         video_input and image_inputs is returned by process_vision_info\n",
        "        \n",
        "#         video_inputs: List[torch.Tensor[Int]] | List[torch.Tensor[Float]] | List[List[PIL.Image]]\n",
        "#         image_inputs: List[PIL.Image]\n",
        "#     \"\"\"\n",
        "#     video_tchw = []\n",
        "#     image_tchw = []\n",
        "\n",
        "#     if video_inputs:\n",
        "#         for video_input in video_inputs:\n",
        "#             if isinstance(video_input, torch.Tensor):\n",
        "#                 video_input = video_input.float() / 255.0  # Normalize to [0, 1]\n",
        "#             elif isinstance(video_input, list) and all(isinstance(img, Image.Image) for img in video_input):\n",
        "#                 # Convert list of PIL Images to tensor\n",
        "#                 video_input = torch.stack([torch.tensor(np.array(img)).permute(2, 0, 1) for img in video_input]).float() / 255.0\n",
        "#             else:\n",
        "#                 raise ValueError(\"Unsupported video input format.\")\n",
        "#             video_tchw.append(video_input)\n",
        "    \n",
        "#     if image_inputs:\n",
        "#         for image_input in image_inputs:\n",
        "#             if isinstance(image_input, Image.Image):\n",
        "#                 image_input = torch.tensor(np.array(image_input)).permute(2, 0, 1).float() / 255.0\n",
        "#             else:\n",
        "#                 raise ValueError(\"Unsupported image input format.\")\n",
        "#             image_tchw.append(image_input)\n",
        "\n",
        "#     batch.update({\n",
        "#         \"video_tchw\": video_tchw if video_tchw else None,\n",
        "#         \"image_tchw\": image_tchw if image_tchw else None,\n",
        "#     })\n",
        "\n",
        "#     return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # JJ: Define load_model_and_processor function\n",
        "# def load_model_and_processor(model_type: str, model_path: str):\n",
        "#     \"\"\"Load model and processor based on type.\"\"\"\n",
        "#     if \"spatial-mllm\" in model_type:\n",
        "#         from transformers import Qwen2_5_VLProcessor\n",
        "\n",
        "#         from src.qwenvl.model.spatial_mllm import SpatialMLLMConfig, SpatialMLLMForConditionalGeneration\n",
        "\n",
        "#         config = SpatialMLLMConfig.from_pretrained(model_path)\n",
        "#         model = SpatialMLLMForConditionalGeneration.from_pretrained(\n",
        "#             model_path,\n",
        "#             config=config,\n",
        "#             torch_dtype=\"bfloat16\",\n",
        "#             device_map=\"cuda\",\n",
        "#             attn_implementation=\"flash_attention_2\",\n",
        "#         )\n",
        "#         processor = Qwen2_5_VLProcessor.from_pretrained(model_path, use_fast=True)\n",
        "#         return model, processor\n",
        "\n",
        "#     if \"qwen2.5-vl\" in model_type:\n",
        "#         from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
        "\n",
        "#         model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "#             model_path,\n",
        "#             torch_dtype=\"bfloat16\",\n",
        "#             device_map=\"cuda\",\n",
        "#             attn_implementation=\"flash_attention_2\",\n",
        "#         )\n",
        "#         processor = Qwen2_5_VLProcessor.from_pretrained(model_path, use_fast=True)\n",
        "#         return model, processor\n",
        "\n",
        "#     raise ValueError(f\"Unknown model type: {model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JJ: Configuration parameters (edit here as needed)\n",
        "MODEL_PATH = \"Diankun/Spatial-MLLM-v1.1-Instruct-135K\"\n",
        "MODEL_TYPE = \"spatial-mllm\"\n",
        "VIDEO_PATH = \"datasets/fool_mllm/42446103_fool\"  # Can be video file or image folder\n",
        "TEXT = \"How many chair(s) are in this room?\\nPlease answer the question using a single word or phrase.\"\n",
        "MP4_NFRAMES = 16  # Number of frames to sample from mp4 video (only for video files, not image folders)\n",
        "\n",
        "# Generation parameters\n",
        "MAX_NEW_TOKENS = 1024\n",
        "DO_SAMPLE = True\n",
        "TEMPERATURE = 0.1\n",
        "TOP_P = 0.001\n",
        "USE_CACHE = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JJ: Run inference\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the model\n",
        "model, processor = load_model_and_processor(MODEL_TYPE, MODEL_PATH)\n",
        "\n",
        "# Handle both video file and image folder\n",
        "video_path_obj = Path(VIDEO_PATH)\n",
        "\n",
        "if video_path_obj.is_file():  # Video file (.mp4, etc.)\n",
        "    video_content = {\n",
        "        \"type\": \"video\",\n",
        "        \"video\": VIDEO_PATH,\n",
        "        \"nframes\": MP4_NFRAMES,\n",
        "    }\n",
        "elif video_path_obj.is_dir():  # Image folder (pretend as video)\n",
        "    image_files = sorted(glob.glob(str(video_path_obj / \"*.png\")))\n",
        "    if not image_files:\n",
        "        raise FileNotFoundError(f\"No PNG files found in {VIDEO_PATH}\")\n",
        "    \n",
        "    video_content = {\n",
        "        \"type\": \"video\",\n",
        "        \"video\": image_files,  # List of image paths\n",
        "        # Note: Do NOT set nframes for image list\n",
        "    }\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Path not found: {VIDEO_PATH}\")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            video_content,\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": TEXT,\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preparation for inference\n",
        "prompts_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "batch = processor(\n",
        "    text=[prompts_text],\n",
        "    images=image_inputs if image_inputs else None,\n",
        "    videos=video_inputs if video_inputs else None,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    padding_side=\"left\",\n",
        ")\n",
        "\n",
        "if \"spatial-mllm\" in MODEL_TYPE:\n",
        "    batch = prepare_spatial_mllm_inputs(batch, video_inputs, image_inputs)\n",
        "\n",
        "batch.to(model.device)\n",
        "if \"image_tchw\" in batch and batch[\"image_tchw\"] is not None:\n",
        "    batch[\"image_tchw\"] = [image_tchw_i.to(model.device) for image_tchw_i in batch[\"image_tchw\"]]\n",
        "if \"video_tchw\" in batch and batch[\"video_tchw\"] is not None:\n",
        "    batch[\"video_tchw\"] = [video_tchw_i.to(model.device) for video_tchw_i in batch[\"video_tchw\"]]\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    do_sample=DO_SAMPLE,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    use_cache=USE_CACHE,\n",
        ")\n",
        "\n",
        "# Start time measurement\n",
        "time_0 = time.time()\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**batch, **generation_kwargs)\n",
        "time_taken = time.time() - time_0\n",
        "\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch[\"input_ids\"], generated_ids)\n",
        "]\n",
        "num_generated_tokens = sum(len(ids) for ids in generated_ids_trimmed)\n",
        "\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(f\"Time taken for inference: {time_taken:.2f} seconds\")\n",
        "print(f\"GPU Memory taken for inference: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"Number of generated tokens: {num_generated_tokens}\")\n",
        "print(f\"Time taken per token: {time_taken / num_generated_tokens:.4f} seconds/token\")\n",
        "print(f\"Output: {output_text}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spatial-mllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
