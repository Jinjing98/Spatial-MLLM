{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RoPE Position Calculation in Spatial-MLLM\n",
        "\n",
        "This notebook demonstrates how the model calculates `position_ids` during:\n",
        "\n",
        "1. **Pre-fill stage** (first forward pass, `cache_position[0] == 0`)\n",
        "2. **Generation stage** (subsequent forward passes, using cached `rope_deltas`)\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **RoPE (Rotary Position Embedding)**: A way to encode position information\n",
        "- **rope_deltas**: Cached offset values from vision tokens to apply to text tokens\n",
        "- **cache_position**: Current position in the generation sequence\n",
        "- **position_ids**: 3D tensor `[3, batch_size, seq_length]` for temporal/height/width dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Pre-fill Function\n",
        "\n",
        "During pre-fill, we calculate `position_ids` from scratch and cache `rope_deltas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_prefill_stage(\n",
        "    input_ids,\n",
        "    image_grid_thw,\n",
        "    video_grid_thw,\n",
        "    second_per_grid_ts,\n",
        "    attention_mask,\n",
        "    inputs_embeds,\n",
        "):\n",
        "    \"\"\"\n",
        "    Stage 1: Pre-fill (first forward pass)\n",
        "    \n",
        "    During pre-fill, we calculate position_ids from scratch and cache rope_deltas.\n",
        "    \n",
        "    Args:\n",
        "        input_ids: [batch_size, seq_length] - token IDs\n",
        "        image_grid_thw: [num_images, 3] - temporal/height/width grid for images\n",
        "        video_grid_thw: [num_videos, 3] - temporal/height/width grid for videos\n",
        "        second_per_grid_ts: [num_videos] - time interval per grid\n",
        "        attention_mask: [batch_size, seq_length] - attention mask (1=attend, 0=ignore)\n",
        "        inputs_embeds: [batch_size, seq_length, hidden_dim] - input embeddings\n",
        "    \n",
        "    Returns:\n",
        "        position_ids: [3, batch_size, seq_length] - 3D position IDs (temporal, height, width)\n",
        "        rope_deltas: [batch_size] - cached position offsets for future generations\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STAGE 1: PRE-FILL (Initial Forward Pass)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # This would normally call self.get_rope_index()\n",
        "    # For demonstration, we'll create dummy outputs with realistic shapes\n",
        "    \n",
        "    batch_size, seq_length = input_ids.shape\n",
        "    \n",
        "    print(f\"\\nInput dimensions:\")\n",
        "    print(f\"  input_ids: {input_ids.shape}\")\n",
        "    print(f\"  inputs_embeds: {inputs_embeds.shape}\")\n",
        "    print(f\"  attention_mask: {attention_mask.shape}\")\n",
        "    \n",
        "    # Simulate get_rope_index output\n",
        "    # In reality, this would calculate based on vision token positions\n",
        "    # For vision tokens: uses grid_thw to compute 3D positions\n",
        "    # For text tokens: sequential positions starting after vision tokens\n",
        "    \n",
        "    # Example: Let's say we have 12 vision tokens and 5 text tokens\n",
        "    # Vision tokens get 3D positions based on grid layout\n",
        "    # Text tokens get sequential positions starting from max_vision_pos + 1\n",
        "    \n",
        "    position_ids = torch.zeros(3, batch_size, seq_length, dtype=torch.long)\n",
        "    \n",
        "    # Dimension 0: temporal positions\n",
        "    # Dimension 1: height positions  \n",
        "    # Dimension 2: width positions\n",
        "    \n",
        "    # Example for first sample (simplified):\n",
        "    # Vision part (first 12 tokens): 3D grid positions\n",
        "    position_ids[0, 0, :12] = torch.tensor([0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100])  # temporal\n",
        "    position_ids[1, 0, :12] = torch.tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1])  # height\n",
        "    position_ids[2, 0, :12] = torch.tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # width\n",
        "    \n",
        "    # Text part (last 5 tokens): all dimensions get same sequential values\n",
        "    position_ids[0, 0, 12:] = torch.arange(101, 106)  # temporal\n",
        "    position_ids[1, 0, 12:] = torch.arange(101, 106)  # height\n",
        "    position_ids[2, 0, 12:] = torch.arange(101, 106)  # width\n",
        "    \n",
        "    print(f\"\\nCalculated position_ids: {position_ids.shape}\")\n",
        "    print(f\"  Shape: [3, {batch_size}, {seq_length}]\")\n",
        "    print(f\"  Temporal dimension (dim 0): {position_ids[0, 0]}\")\n",
        "    print(f\"  Height dimension (dim 1): {position_ids[1, 0]}\")\n",
        "    print(f\"  Width dimension (dim 2): {position_ids[2, 0]}\")\n",
        "    \n",
        "    # rope_deltas: the position offset where text starts (max vision pos + 1)\n",
        "    # This gets cached for use in generation stage\n",
        "    rope_deltas = torch.tensor([100], dtype=torch.long)  # max vision position was 100\n",
        "    \n",
        "    print(f\"\\nCached rope_deltas: {rope_deltas}\")\n",
        "    print(f\"  Shape: {rope_deltas.shape}\")\n",
        "    print(f\"  This represents the max position from vision tokens\")\n",
        "    print(f\"  Text tokens will continue from this position + cache_position\")\n",
        "    \n",
        "    return position_ids, rope_deltas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Generation Function\n",
        "\n",
        "During generation, we reuse cached `rope_deltas` to calculate `position_ids` efficiently without recalculating vision token positions.\n",
        "\n",
        "### ðŸ”‘ Key Understanding: KV Cache\n",
        "\n",
        "**Question 1**: Where does `inputs_embeds` come from during generation?\n",
        "- **Answer**: Only the **newly generated token** (the last one generated)\n",
        "- Shape: `[batch_size, 1, hidden_dim]` - note seq_length=1!\n",
        "\n",
        "**Question 3**: Does `batch_size, seq_length, hidden_dim = inputs_embeds.shape` include all previous tokens from pre-fill?\n",
        "- **Answer**: **NO!** Only the current new token!\n",
        "- All previous tokens (from pre-fill stage) are stored in KV cache\n",
        "- We don't re-process them - that's the efficiency of KV caching!\n",
        "\n",
        "**Question 2**: What does `repeat_interleave` do?\n",
        "- Handles batched generation where `batch_size` might differ from cached `rope_deltas` size\n",
        "- In our simple demo: batch_size=1, so it's a no-op\n",
        "- Real use case: If you cache for 1 sample but generate for 2 samples in parallel\n",
        "\n",
        "**Flow Example**:\n",
        "```\n",
        "Pre-fill: Process tokens [0-16] â†’ store keys/values in cache â†’ cache rope_deltas\n",
        "Gen step 1: Process ONLY token 17 â†’ position_id = 17 + 100 = 117\n",
        "Gen step 2: Process ONLY token 18 â†’ position_id = 18 + 100 = 118\n",
        "Gen step 3: Process ONLY token 19 â†’ position_id = 19 + 100 = 119\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_generation_stage(\n",
        "    cached_rope_deltas,\n",
        "    cache_position,\n",
        "    inputs_embeds,\n",
        "):\n",
        "    \"\"\"\n",
        "    Stage 2: Generation (subsequent forward passes during decoding)\n",
        "    \n",
        "    During generation, we reuse cached rope_deltas to calculate position_ids efficiently.\n",
        "    We don't need to recalculate vision token positions.\n",
        "    \n",
        "    Args:\n",
        "        cached_rope_deltas: [batch_size] - cached offset from pre-fill stage\n",
        "        cache_position: [seq_length] - current positions in generation (e.g., [17], [18], ...)\n",
        "        inputs_embeds: [batch_size, seq_length, hidden_dim] - ONLY THE NEW TOKEN! (seq_length=1)\n",
        "                       All previous tokens are in KV cache, we don't reprocess them!\n",
        "    \n",
        "    Returns:\n",
        "        position_ids: [3, batch_size, seq_length] - 3D position IDs for current tokens\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STAGE 2: GENERATION (Subsequent Forward Passes)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Question 3: Does this include all previous tokens from pre-fill?\n",
        "    # Answer: NO! Only the current new token (seq_length=1)\n",
        "    batch_size, seq_length, hidden_dim = inputs_embeds.shape\n",
        "    \n",
        "    print(f\"\\nInput dimensions:\")\n",
        "    print(f\"  inputs_embeds: {inputs_embeds.shape} (typically seq_length=1 during generation)\")\n",
        "    print(f\"  âš ï¸  This is ONLY the newly generated token, not all previous tokens!\")\n",
        "    print(f\"  cache_position: {cache_position.shape} = {cache_position}\")\n",
        "    print(f\"  cached_rope_deltas: {cached_rope_deltas.shape} = {cached_rope_deltas}\")\n",
        "    \n",
        "    # Calculate delta: current position + rope offset\n",
        "    delta = (cache_position[0] + cached_rope_deltas).to(inputs_embeds.device)\n",
        "    print(f\"\\nCalculated delta: {delta}\")\n",
        "    print(f\"  delta = cache_position[0] ({cache_position[0]}) + rope_deltas ({cached_rope_deltas[0]}) = {delta[0]}\")\n",
        "    \n",
        "    # Create base position IDs for current sequence\n",
        "    # For generation with seq_length=1, this is just [0]\n",
        "    position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n",
        "    print(f\"\\nBase position_ids: {position_ids}\")\n",
        "    print(f\"  This is just [0] since we're processing 1 token\")\n",
        "    \n",
        "    # Expand to batch size: [batch_size, seq_length]\n",
        "    position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n",
        "    print(f\"\\nExpanded to batch: {position_ids.shape} = {position_ids}\")\n",
        "    \n",
        "    # Question 2: What does repeat_interleave do here?\n",
        "    # Answer: Handle cases where batch_size > cached_rope_deltas.shape[0]\n",
        "    # Example: If we cached deltas for 1 sample but now generate for 2 samples\n",
        "    # In our demo: batch_size=1, delta.shape[0]=1, so this just keeps delta as-is\n",
        "    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n",
        "    print(f\"\\nDelta repeated for batch: {delta.shape} = {delta}\")\n",
        "    print(f\"  (In this demo: 1 // 1 = 1, so no actual repetition happens)\")\n",
        "    \n",
        "    # Add delta to get final positions\n",
        "    # position_ids was [0], after adding delta it becomes [delta]\n",
        "    position_ids = position_ids.add(delta)\n",
        "    print(f\"\\nAfter adding delta: {position_ids.shape} = {position_ids}\")\n",
        "    print(f\"  [0] + {delta[0].item()} = [{position_ids[0, 0].item()}]\")\n",
        "    \n",
        "    # Expand to 3D (temporal, height, width dimensions all get same values)\n",
        "    position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n",
        "    print(f\"\\nFinal position_ids: {position_ids.shape}\")\n",
        "    print(f\"  Shape: [3, {batch_size}, {seq_length}]\")\n",
        "    print(f\"  All dimensions: {position_ids[:, 0, 0]}\")\n",
        "    print(f\"  This means: temporal={position_ids[0, 0, 0]}, height={position_ids[1, 0, 0]}, width={position_ids[2, 0, 0]}\")\n",
        "    \n",
        "    return position_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual Explanation\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                      PRE-FILL STAGE                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Input: ALL tokens [0-16] (12 vision + 5 text)                 â”‚\n",
        "â”‚  Process: inputs_embeds.shape = [1, 17, 3584]                  â”‚\n",
        "â”‚  Output: position_ids for all 17 tokens                        â”‚\n",
        "â”‚         Store K,V in cache for all tokens                      â”‚\n",
        "â”‚         Cache rope_deltas = 100                                â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                            â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   GENERATION STEP 1                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Input: ONLY new token [17]                                    â”‚\n",
        "â”‚  Process: inputs_embeds.shape = [1, 1, 3584]  â† seq_len=1!    â”‚\n",
        "â”‚  Compute: position_ids = 0 + (17 + 100) = 117                 â”‚\n",
        "â”‚  KV Cache: Append this token's K,V to existing cache          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                            â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   GENERATION STEP 2                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Input: ONLY new token [18]                                    â”‚\n",
        "â”‚  Process: inputs_embeds.shape = [1, 1, 3584]  â† seq_len=1!    â”‚\n",
        "â”‚  Compute: position_ids = 0 + (18 + 100) = 118                 â”‚\n",
        "â”‚  KV Cache: Append this token's K,V to existing cache          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Key Point: We NEVER reprocess tokens 0-16 after pre-fill!\n",
        "           Their K,V are cached and reused!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Create Fake Inputs\n",
        "\n",
        "Let's create example inputs to demonstrate the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "prompt_seq_length = 17  # e.g., 12 vision tokens + 5 text tokens\n",
        "hidden_dim = 3584\n",
        "\n",
        "# Pre-fill stage inputs\n",
        "input_ids = torch.randint(0, 1000, (batch_size, prompt_seq_length))\n",
        "attention_mask = torch.ones(batch_size, prompt_seq_length)\n",
        "inputs_embeds = torch.randn(batch_size, prompt_seq_length, hidden_dim)\n",
        "\n",
        "# Vision grid info (example: video with 3 temporal patches, 2x2 spatial)\n",
        "video_grid_thw = torch.tensor([[3, 2, 2]])  # [num_videos, 3]\n",
        "image_grid_thw = None\n",
        "second_per_grid_ts = torch.tensor([1.0])  # 1 second per temporal grid\n",
        "\n",
        "print(f\"Pre-fill inputs created:\")\n",
        "print(f\"  batch_size: {batch_size}\")\n",
        "print(f\"  prompt_seq_length: {prompt_seq_length}\")\n",
        "print(f\"  hidden_dim: {hidden_dim}\")\n",
        "print(f\"  video_grid_thw: {video_grid_thw} (temporal=3, height=2, width=2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Stage 1: Pre-fill\n",
        "\n",
        "This is the first forward pass where we calculate position IDs from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_position = torch.tensor([0])  # First token position\n",
        "\n",
        "# Condition check from the code\n",
        "is_prefill = (cache_position is not None and cache_position[0] == 0)\n",
        "\n",
        "print(f\"Checking if we're in pre-fill stage:\")\n",
        "print(f\"  cache_position: {cache_position}\")\n",
        "print(f\"  cache_position[0] == 0: {is_prefill}\")\n",
        "print(f\"  -> Will calculate position_ids from scratch\")\n",
        "\n",
        "position_ids_prefill, rope_deltas = simulate_prefill_stage(\n",
        "    input_ids=input_ids,\n",
        "    image_grid_thw=image_grid_thw,\n",
        "    video_grid_thw=video_grid_thw,\n",
        "    second_per_grid_ts=second_per_grid_ts,\n",
        "    attention_mask=attention_mask,\n",
        "    inputs_embeds=inputs_embeds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Stage 2: Generation (3 steps)\n",
        "\n",
        "Now simulate token generation where we use the cached `rope_deltas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in range(3):  # Simulate 3 generation steps\n",
        "    # During generation, seq_length=1 (one token at a time)\n",
        "    gen_seq_length = 1\n",
        "    gen_inputs_embeds = torch.randn(batch_size, gen_seq_length, hidden_dim)\n",
        "    gen_cache_position = torch.tensor([prompt_seq_length + step])\n",
        "    \n",
        "    print(f\"\\n{'â”€'*80}\")\n",
        "    print(f\"Generation Step {step + 1}\")\n",
        "    print(f\"{'â”€'*80}\")\n",
        "    print(f\"Generating token at position: {gen_cache_position[0]}\")\n",
        "    \n",
        "    position_ids_gen = simulate_generation_stage(\n",
        "        cached_rope_deltas=rope_deltas,\n",
        "        cache_position=gen_cache_position,\n",
        "        inputs_embeds=gen_inputs_embeds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "**1. Pre-fill stage:**\n",
        "- Calculates full 3D position_ids for vision tokens (temporal, height, width)\n",
        "- Text tokens get sequential 1D positions (same across all 3 dims)\n",
        "- Caches rope_deltas (max vision position) for later use\n",
        "\n",
        "**2. Generation stage:**\n",
        "- Uses cached rope_deltas to avoid recalculating vision positions\n",
        "- `position_ids = cache_position[0] + rope_deltas`\n",
        "- Much faster since it's just arithmetic, no grid calculations\n",
        "\n",
        "**3. Why 3D position IDs?**\n",
        "- **Temporal**: encodes time/frame information\n",
        "- **Height**: encodes vertical spatial position\n",
        "- **Width**: encodes horizontal spatial position\n",
        "- Allows RoPE to understand spatial-temporal relationships in video/images\n",
        "\n",
        "**4. Key dimensions:**\n",
        "- `position_ids`: `[3, batch_size, seq_length]`\n",
        "  - First dim (3): temporal, height, width\n",
        "  - Used by RoPE to encode positional information\n",
        "- `rope_deltas`: `[batch_size]`\n",
        "  - Scalar offset per batch\n",
        "  - Represents where text generation continues from"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
